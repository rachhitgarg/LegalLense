"""
faiss_search.py - Pre-computed FAISS search engine for Legal Lens.

This uses a pre-built FAISS index generated by build_index.py.
No heavy ML dependencies required at runtime - just loads the index!

To add new documents:
1. Add PDF files to backend/data/judgments/
2. Run: python build_index.py
3. Commit and push the changes
"""

import json
import numpy as np
from pathlib import Path
from typing import List, Dict, Optional
from dataclasses import dataclass


@dataclass
class SearchResult:
    doc_id: str
    title: str
    content: str
    score: float
    source: str
    metadata: Dict = None


class FAISSSearchEngine:
    """
    Pre-computed FAISS search engine.
    
    Uses a pre-built index - no ML model loading required at runtime!
    This allows it to work on memory-constrained hosting like Render free tier.
    """
    
    def __init__(self, data_dir: str = None):
        # Use absolute path based on this file's location
        if data_dir is None:
            self.data_dir = Path(__file__).parent.parent / "data"
        else:
            self.data_dir = Path(data_dir)
        
        self.documents = []
        self.index = None
        self.embeddings = None
        
        # Load pre-built data
        self._load_index()
    
    def _load_index(self):
        """Load pre-built FAISS index and documents."""
        docs_file = self.data_dir / "documents.json"
        index_file = self.data_dir / "faiss.index"
        embeddings_file = self.data_dir / "embeddings.npy"
        
        # Load documents
        if docs_file.exists():
            with open(docs_file, "r", encoding="utf-8") as f:
                self.documents = json.load(f)
            print(f"[Search] Loaded {len(self.documents)} documents")
        else:
            print(f"[Search] No documents file found at {docs_file}")
            self._create_sample_documents()
        
        # Try to load FAISS index
        if index_file.exists():
            try:
                import faiss
                self.index = faiss.read_index(str(index_file))
                print(f"[Search] Loaded FAISS index with {self.index.ntotal} vectors")
                return
            except ImportError:
                print("[Search] faiss-cpu not installed, trying numpy fallback")
            except Exception as e:
                print(f"[Search] Could not load FAISS index: {e}")
        
        # Fallback: load numpy embeddings
        if embeddings_file.exists():
            self.embeddings = np.load(embeddings_file)
            print(f"[Search] Loaded numpy embeddings: {self.embeddings.shape}")
        else:
            print("[Search] No pre-built index found. Using keyword search.")
    
    def _create_sample_documents(self):
        """Create sample documents if none exist."""
        self.documents = [
            {
                "doc_id": "jacob_mathew_2005",
                "title": "Jacob Mathew vs State of Punjab (2005)",
                "content": "This landmark case established the comprehensive law on medical negligence in India. The Supreme Court held that a medical professional can only be held liable for negligence if it is established that he did not possess the requisite skill or did not exercise reasonable care.",
                "keywords": ["medical negligence", "doctor liability", "malpractice"],
                "statutes": ["IPC 304A", "BNS 106"],
                "source": "sample"
            },
            {
                "doc_id": "puttaswamy_2017",
                "title": "K.S. Puttaswamy vs Union of India (2017)",
                "content": "The landmark Right to Privacy judgment. A nine-judge Constitution Bench unanimously held that right to privacy is a fundamental right intrinsic to Article 21.",
                "keywords": ["privacy", "fundamental right", "article 21"],
                "statutes": ["Article 21", "Article 14"],
                "source": "sample"
            },
            {
                "doc_id": "navtej_johar_2018",
                "title": "Navtej Singh Johar vs Union of India (2018)",
                "content": "The Supreme Court decriminalized homosexuality by reading down Section 377 of the Indian Penal Code. Consensual sexual conduct between adults of the same sex in private is not a crime.",
                "keywords": ["section 377", "homosexuality", "LGBTQ", "decriminalization"],
                "statutes": ["IPC 377", "Article 14", "Article 21"],
                "source": "sample"
            }
        ]
        
        # Save for future use
        self.data_dir.mkdir(parents=True, exist_ok=True)
        with open(self.data_dir / "documents.json", "w", encoding="utf-8") as f:
            json.dump(self.documents, f, indent=2)
    
    def search(self, query: str, top_k: int = 5) -> List[SearchResult]:
        """
        Search using pre-computed FAISS index or numpy embeddings.
        Falls back to keyword search if no embeddings available.
        """
        # If we have FAISS index, use it
        if self.index is not None:
            return self._faiss_search(query, top_k)
        
        # If we have numpy embeddings, use them
        if self.embeddings is not None:
            return self._numpy_search(query, top_k)
        
        # Fallback to keyword search
        return self._keyword_search(query, top_k)
    
    def _faiss_search(self, query: str, top_k: int) -> List[SearchResult]:
        """Search using FAISS index."""
        try:
            from sentence_transformers import SentenceTransformer
            import faiss
            
            # Load model for query encoding
            model = SentenceTransformer("all-MiniLM-L6-v2")
            query_embedding = model.encode([query])
            faiss.normalize_L2(query_embedding)
            
            # Search
            scores, indices = self.index.search(query_embedding, min(top_k, len(self.documents)))
            
            results = []
            for score, idx in zip(scores[0], indices[0]):
                if idx < 0 or idx >= len(self.documents):
                    continue
                doc = self.documents[idx]
                results.append(self._make_result(doc, float(score), "faiss"))
            
            return results
            
        except ImportError:
            return self._keyword_search(query, top_k)
    
    def _numpy_search(self, query: str, top_k: int) -> List[SearchResult]:
        """Search using numpy embeddings (no FAISS required)."""
        try:
            from sentence_transformers import SentenceTransformer
            
            # Load model for query encoding
            model = SentenceTransformer("all-MiniLM-L6-v2")
            query_embedding = model.encode([query])
            query_embedding = query_embedding / np.linalg.norm(query_embedding)
            
            # Compute cosine similarity
            scores = np.dot(self.embeddings, query_embedding.T).flatten()
            
            # Get top-k indices
            top_indices = np.argsort(scores)[::-1][:top_k]
            
            results = []
            for idx in top_indices:
                if idx < len(self.documents):
                    doc = self.documents[idx]
                    results.append(self._make_result(doc, float(scores[idx]), "numpy"))
            
            return results
            
        except ImportError:
            return self._keyword_search(query, top_k)
    
    def _keyword_search(self, query: str, top_k: int) -> List[SearchResult]:
        """Fallback keyword search (no ML dependencies)."""
        query_lower = query.lower()
        query_words = [w for w in query_lower.split() if len(w) > 2]
        
        scored_docs = []
        
        for doc in self.documents:
            score = 0.0
            
            title = doc.get("title", "").lower()
            content = doc.get("content", "").lower()
            keywords = [k.lower() for k in doc.get("keywords", [])]
            
            for word in query_words:
                # Title match (highest weight)
                if word in title:
                    score += 0.3
                
                # Keyword match
                for kw in keywords:
                    if word in kw or kw in word:
                        score += 0.25
                
                # Content match
                score += min(content.count(word) * 0.02, 0.2)
            
            if score > 0:
                scored_docs.append((doc, score))
        
        # Sort by score
        scored_docs.sort(key=lambda x: x[1], reverse=True)
        
        results = []
        for doc, score in scored_docs[:top_k]:
            results.append(self._make_result(doc, min(score, 1.0), "keyword"))
        
        return results
    
    def _make_result(self, doc: Dict, score: float, source: str) -> SearchResult:
        """Create SearchResult from document."""
        return SearchResult(
            doc_id=doc["doc_id"],
            title=doc.get("title", ""),
            content=doc.get("content", "")[:500],
            score=score,
            source=source,
            metadata={
                "year": doc.get("year"),
                "court": doc.get("court"),
                "statutes": doc.get("statutes", []),
                "filename": doc.get("filename")
            }
        )
    
    def get_document_count(self) -> int:
        """Get number of indexed documents."""
        return len(self.documents)


# Singleton instance
_search_engine = None

def get_search_engine(data_dir: str = None) -> FAISSSearchEngine:
    """Get or create the FAISS search engine instance."""
    global _search_engine
    if _search_engine is None:
        _search_engine = FAISSSearchEngine(data_dir)
    return _search_engine
